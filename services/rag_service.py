"""RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æœå‹™å±¤"""
from typing import List, Optional
from langchain_core.documents import Document
from .document_service import DocumentService
from .vector_store_service import VectorStoreService
from .llm_service import LLMService


class RAGService:
    """RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æœå‹™ - æ•´åˆæ–‡æª”è™•ç†ã€å‘é‡æª¢ç´¢å’Œ LLM ç”Ÿæˆ"""
    
    def __init__(
        self, 
        document_service: DocumentService,
        vector_store_service: VectorStoreService,
        llm_service: LLMService,
        default_k: int = 4
    ):
        """
        åˆå§‹åŒ– RAG æœå‹™
        
        Args:
            document_service: æ–‡æª”è™•ç†æœå‹™
            vector_store_service: å‘é‡å­˜å„²æœå‹™
            llm_service: LLM æœå‹™
            default_k: é è¨­æª¢ç´¢çš„æ–‡æª”æ•¸é‡
        """
        self.doc_service = document_service
        self.vector_service = vector_store_service
        self.llm_service = llm_service
        self.default_k = default_k
    
    def ingest_file(self, file_path: str) -> dict:
        """
        æ”å…¥æ–‡ä»¶åˆ°çŸ¥è­˜åº«
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾‘
            
        Returns:
            åŒ…å«è™•ç†çµæœçš„å­—å…¸
        """
        # è™•ç†æ–‡ä»¶ï¼ˆåŠ è¼‰ + åˆ†å‰²ï¼‰
        chunks = self.doc_service.process_file(file_path)
        
        # æ·»åŠ åˆ°å‘é‡å­˜å„²
        ids = self.vector_service.add_documents(chunks)
        
        return {
            "chunks_count": len(chunks),
            "document_ids": ids,
            "file_path": file_path
        }
    
    def query_with_context(
        self, 
        query: str, 
        k: Optional[int] = None,
        use_mmr: bool = False,
        include_sources: bool = True
    ) -> str:
        """
        ä½¿ç”¨æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”å•é¡Œ
        
        Args:
            query: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„æ–‡æª”æ•¸é‡ï¼ˆNone å‰‡ä½¿ç”¨é è¨­å€¼ï¼‰
            use_mmr: æ˜¯å¦ä½¿ç”¨æœ€å¤§é‚Šéš›ç›¸é—œæ€§æœç´¢ï¼ˆé¿å…é‡è¤‡å…§å®¹ï¼‰
            include_sources: æ˜¯å¦åœ¨å›ç­”ä¸­åŒ…å«ä¾†æºä¿¡æ¯
            
        Returns:
            AI å›ç­”
        """
        k = k or self.default_k
        
        # 1. æª¢ç´¢ç›¸é—œæ–‡æª”
        if use_mmr:
            relevant_docs = self.vector_service.max_marginal_relevance_search(query, k=k)
        else:
            relevant_docs = self.vector_service.similarity_search(query, k=k)
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”
        if not relevant_docs:
            return self.llm_service.send_message(
                f"{query}\n\nï¼ˆæ³¨æ„ï¼šçŸ¥è­˜åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼Œä»¥ä¸‹æ˜¯åŸºæ–¼æ¨¡å‹çŸ¥è­˜çš„å›ç­”ï¼‰"
            )
        
        # 2. æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._format_context(relevant_docs)
        
        # 3. æ§‹å»ºæç¤ºè©
        prompt = self._build_prompt(query, context)
        
        # 4. èª¿ç”¨ LLMï¼ˆæœƒè‡ªå‹•ä½¿ç”¨æ­·å²è¨˜æ†¶ï¼‰
        response = self.llm_service.send_message(prompt)
        
        # 5. å¦‚æœéœ€è¦ï¼Œæ·»åŠ ä¾†æºä¿¡æ¯
        if include_sources:
            sources = self._format_sources(relevant_docs)
            response = f"{response}\n\n{sources}"
        
        return response
    
    def query_with_score(
        self, 
        query: str, 
        k: Optional[int] = None,
        score_threshold: float = 1.5
    ) -> str:
        """
        ä½¿ç”¨å¸¶åˆ†æ•¸çš„æª¢ç´¢ï¼ˆå¯ä»¥éæ¿¾ä¸ç›¸é—œçš„çµæœï¼‰
        
        Args:
            query: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„æ–‡æª”æ•¸é‡
            score_threshold: ç›¸ä¼¼åº¦åˆ†æ•¸é–¾å€¼ï¼ˆè¶Šä½è¶Šåš´æ ¼ï¼‰
            
        Returns:
            AI å›ç­”
        """
        k = k or self.default_k
        
        # æª¢ç´¢å¸¶åˆ†æ•¸çš„æ–‡æª”
        results = self.vector_service.similarity_search_with_score(query, k=k)
        
        # éæ¿¾ä½åˆ†æ–‡æª”
        filtered_docs = [doc for doc, score in results if score <= score_threshold]
        
        if not filtered_docs:
            return self.llm_service.send_message(
                f"{query}\n\nï¼ˆæ³¨æ„ï¼šçŸ¥è­˜åº«ä¸­æ²’æœ‰æ‰¾åˆ°è¶³å¤ ç›¸é—œçš„è³‡æ–™ï¼‰"
            )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡ä¸¦ç”Ÿæˆå›ç­”
        context = self._format_context(filtered_docs)
        prompt = self._build_prompt(query, context)
        response = self.llm_service.send_message(prompt)
        
        return response
    
    def _format_context(self, documents: List[Document]) -> str:
        """
        æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡æª”ä½œç‚ºä¸Šä¸‹æ–‡
        
        Args:
            documents: æ–‡æª”åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            # æå–å…ƒæ•¸æ“š
            source = doc.metadata.get('source', 'Unknown')
            page = doc.metadata.get('page', '')
            
            # æ ¼å¼åŒ–æ–‡æª”å¡Š
            source_info = f"ä¾†æº: {source}"
            if page:
                source_info += f", ç¬¬ {page} é "
            
            context_parts.append(
                f"[æ–‡æª”ç‰‡æ®µ {i}] ({source_info})\n{doc.page_content}\n"
            )
        
        return "\n".join(context_parts)
    
    def _format_sources(self, documents: List[Document]) -> str:
        """
        æ ¼å¼åŒ–ä¾†æºä¿¡æ¯
        
        Args:
            documents: æ–‡æª”åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ä¾†æºä¿¡æ¯
        """
        # å»é‡ä¾†æº
        sources = set()
        for doc in documents:
            source = doc.metadata.get('source', 'Unknown')
            sources.add(source)
        
        sources_list = list(sources)
        if len(sources_list) == 1:
            return f"ğŸ“š **è³‡æ–™ä¾†æºï¼š** {sources_list[0]}"
        else:
            sources_str = "\n".join([f"  - {s}" for s in sources_list])
            return f"ğŸ“š **è³‡æ–™ä¾†æºï¼š**\n{sources_str}"
    
    def _build_prompt(self, query: str, context: str) -> str:
        """
        æ§‹å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºè©
        
        Args:
            query: ç”¨æˆ¶å•é¡Œ
            context: æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            å®Œæ•´çš„æç¤ºè©
        """
        return f"""è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¾†å›ç­”å•é¡Œã€‚

**é‡è¦è¦å‰‡ï¼š**
1. åƒ…åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹æ˜ç¢ºèªªæ˜
3. ä¸è¦ç·¨é€ æˆ–æ¨æ¸¬ä¸Šä¸‹æ–‡ä¸­æ²’æœ‰çš„ä¿¡æ¯
4. å¯ä»¥æ•´åˆå¤šå€‹æ–‡æª”ç‰‡æ®µçš„ä¿¡æ¯

**ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š**
{context}

**å•é¡Œï¼š** {query}

**å›ç­”ï¼š**"""
    
    def get_knowledge_base_stats(self) -> dict:
        """
        ç²å–çŸ¥è­˜åº«çµ±è¨ˆä¿¡æ¯
        
        Returns:
            çµ±è¨ˆä¿¡æ¯å­—å…¸
        """
        store_info = self.vector_service.get_store_info()
        
        return {
            "total_chunks": store_info["total_documents"],
            "collection_name": store_info["collection_name"],
            "embedding_model": store_info["embedding_model"],
            "supported_formats": self.doc_service.get_supported_formats()
        }
    
    def clear_knowledge_base(self) -> None:
        """æ¸…ç©ºæ•´å€‹çŸ¥è­˜åº«"""
        self.vector_service.delete_collection()
    
    def search_documents(self, query: str, k: int = 4) -> List[Document]:
        """
        åƒ…æœç´¢æ–‡æª”ï¼Œä¸ç”Ÿæˆå›ç­”ï¼ˆç”¨æ–¼é è¦½ç›¸é—œæ–‡æª”ï¼‰
        
        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æª”æ•¸é‡
            
        Returns:
            ç›¸é—œæ–‡æª”åˆ—è¡¨
        """
        return self.vector_service.similarity_search(query, k=k)
