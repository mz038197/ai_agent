"""
Chainlit UI å±¤
è² è²¬è™•ç†ç”¨æˆ¶ç•Œé¢äº¤äº’ï¼Œå°‡æ¥­å‹™é‚è¼¯å§”æ‰˜çµ¦æœå‹™å±¤
éµå®ˆå–®ä¸€è·è²¬åŸå‰‡ (Single Responsibility Principle)
"""
import chainlit as cl
from services import (
    LLMService, 
    ImageService,
    DocumentService,
    VectorStoreService,
    RAGService
)


# é…ç½®åƒæ•¸
CONFIG = {
    "MODEL": "gemma3:4b",
    "BASE_URL": "http://localhost:11434",
    "TEMPERATURE": 0.7,
    "EMBEDDING_MODEL": "nomic-embed-text",
    "CHROMA_DB_PATH": "./chroma_db",
    "SYSTEM_PROMPT": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„ AI åŠ©æ‰‹ï¼Œå…·å‚™ä»¥ä¸‹ç‰¹é»ï¼š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
- æä¾›æº–ç¢ºã€æ¸…æ™°ã€æœ‰å¹«åŠ©çš„å›ç­”
- ç•¶è™•ç†æ–‡æª”ç›¸é—œå•é¡Œæ™‚ï¼Œåš´æ ¼åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”
- å¦‚æœä¸ç¢ºå®šæˆ–ä¿¡æ¯ä¸è¶³ï¼Œæœƒæ˜ç¢ºèªªæ˜
- ä»¥å°ˆæ¥­ä½†è¦ªåˆ‡çš„èªæ°£èˆ‡ç”¨æˆ¶äº¤æµ"""
}


@cl.on_chat_start
async def start():
    """åˆå§‹åŒ–èŠå¤©æœƒè©±"""
    # åˆå§‹åŒ– LLM æœå‹™
    llm_service = LLMService(
        model=CONFIG["MODEL"],
        base_url=CONFIG["BASE_URL"],
        temperature=CONFIG["TEMPERATURE"],
        system_prompt=CONFIG["SYSTEM_PROMPT"]
    )
    
    # åˆå§‹åŒ– RAG ç›¸é—œæœå‹™
    doc_service = DocumentService(chunk_size=1000, chunk_overlap=200)
    vector_service = VectorStoreService(
        persist_directory=CONFIG["CHROMA_DB_PATH"],
        embedding_model=CONFIG["EMBEDDING_MODEL"],
        base_url=CONFIG["BASE_URL"]
    )
    rag_service = RAGService(
        document_service=doc_service,
        vector_store_service=vector_service,
        llm_service=llm_service,
        default_k=4
    )
    
    # å°‡æœå‹™å­˜å„²åœ¨ç”¨æˆ¶æœƒè©±ä¸­
    cl.user_session.set("llm_service", llm_service)
    cl.user_session.set("rag_service", rag_service)
    
    # ç²å–æ¨¡å‹å’ŒçŸ¥è­˜åº«ä¿¡æ¯
    model_info = llm_service.get_model_info()
    kb_stats = rag_service.get_knowledge_base_stats()
    
    # æ­¡è¿è¨Šæ¯
    await cl.Message(
        content=f"ğŸ‘‹ æ­¡è¿ä½¿ç”¨ AI åŠ©æ‰‹ï¼\n\n"
                f"ğŸ“¦ **ç•¶å‰æ¨¡å‹:** {model_info['model']}\n"
                f"ğŸ“š **çŸ¥è­˜åº«:** {kb_stats['total_chunks']} å€‹æ–‡æª”å¡Š\n\n"
                f"ğŸ’¬ **æ‚¨å¯ä»¥ï¼š**\n"
                f"- ğŸ’­ è¼¸å…¥æ–‡å­—é€²è¡Œå°è©±ï¼ˆè‡ªå‹•ä½¿ç”¨çŸ¥è­˜åº«ï¼‰\n"
                f"- ğŸ“„ ä¸Šå‚³æ–‡ä»¶ï¼ˆPDF/TXT/Markdownï¼‰å»ºç«‹çŸ¥è­˜åº«\n"
                f"- ğŸ–¼ï¸ ä¸Šå‚³åœ–ç‰‡é€²è¡Œè¦–è¦ºåˆ†æ\n"
                f"- ğŸ“Š è¼¸å…¥ `/stats` æŸ¥çœ‹çŸ¥è­˜åº«çµ±è¨ˆ\n"
                f"- ğŸ—‘ï¸ è¼¸å…¥ `/clear` æ¸…ç©ºçŸ¥è­˜åº«",
    ).send()


@cl.on_message
async def handle_message(message: cl.Message):
    """
    è™•ç†ç”¨æˆ¶è¨Šæ¯ï¼ˆçµ±ä¸€è™•ç†æ–‡å­—ã€åœ–ç‰‡å’Œæ–‡æª”ï¼‰
    UIå±¤åªè² è²¬æ¥æ”¶è¼¸å…¥ã€é¡¯ç¤ºè¼¸å‡ºï¼Œæ¥­å‹™é‚è¼¯å§”æ‰˜çµ¦æœå‹™å±¤
    """
    # ç²å–æœå‹™å±¤å¯¦ä¾‹
    llm_service = cl.user_session.get("llm_service")
    rag_service = cl.user_session.get("rag_service")
    
    # æª¢æŸ¥å‘½ä»¤
    if message.content:
        content_lower = message.content.lower().strip()
        
        # çµ±è¨ˆå‘½ä»¤
        if content_lower == "/stats":
            stats = rag_service.get_knowledge_base_stats()
            await cl.Message(
                content=f"ğŸ“Š **çŸ¥è­˜åº«çµ±è¨ˆ**\n\n"
                        f"- æ–‡æª”å¡Šç¸½æ•¸ï¼š{stats['total_chunks']}\n"
                        f"- é›†åˆåç¨±ï¼š{stats['collection_name']}\n"
                        f"- åµŒå…¥æ¨¡å‹ï¼š{stats['embedding_model']}\n"
                        f"- æ”¯æ´æ ¼å¼ï¼š{', '.join(stats['supported_formats'])}"
            ).send()
            return
        
        # æ¸…ç©ºå‘½ä»¤
        if content_lower == "/clear":
            await cl.AskActionMessage(
                content="ç¢ºå®šè¦æ¸…ç©ºæ•´å€‹çŸ¥è­˜åº«å—ï¼Ÿæ­¤æ“ä½œç„¡æ³•æ’¤éŠ·ã€‚",
                actions=[
                    cl.Action(name="confirm", payload={"action": "confirm"}, label="âœ… ç¢ºå®šæ¸…ç©º"),
                    cl.Action(name="cancel", payload={"action": "cancel"}, label="âŒ å–æ¶ˆ"),
                ],
            ).send()
            return
    
    # åˆ†é¡é™„ä»¶
    images = [file for file in message.elements if "image" in file.mime]
    documents = [file for file in message.elements 
                 if file.mime in ["application/pdf", "text/plain", "text/markdown"]
                 or file.name.endswith(('.pdf', '.txt', '.md', '.markdown'))]
    
    try:
        # è™•ç†æ–‡æª”ä¸Šå‚³
        if documents:
            await _handle_document_upload(message, documents, rag_service)
        
        # è™•ç†åœ–ç‰‡
        elif images:
            await _handle_image_message(message, images[0], llm_service)
        
        # è™•ç†ç´”æ–‡å­—ï¼ˆä½¿ç”¨ RAGï¼‰
        else:
            await _handle_text_with_rag(message, rag_service)
            
    except Exception as e:
        await cl.Message(
            content=f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}\n\nè«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ¨¡å‹å·²ä¸‹è¼‰ã€‚"
        ).send()


async def _handle_document_upload(
    message: cl.Message,
    documents: list,
    rag_service: RAGService
):
    """è™•ç†æ–‡æª”ä¸Šå‚³"""
    msg = cl.Message(content="ğŸ“„ æ­£åœ¨è™•ç†æ–‡ä»¶...")
    await msg.send()
    
    results = []
    for doc_file in documents:
        try:
            result = await cl.make_async(rag_service.ingest_file)(doc_file.path)
            results.append(f"âœ… **{doc_file.name}**\n   - å·²æ·»åŠ  {result['chunks_count']} å€‹æ–‡æª”å¡Š")
        except Exception as e:
            results.append(f"âŒ **{doc_file.name}**\n   - éŒ¯èª¤ï¼š{str(e)}")
    
    # ç²å–æ›´æ–°å¾Œçš„çµ±è¨ˆ
    stats = rag_service.get_knowledge_base_stats()
    
    msg.content = "ğŸ“š **æ–‡æª”è™•ç†å®Œæˆ**\n\n" + "\n\n".join(results)
    msg.content += f"\n\nğŸ“Š çŸ¥è­˜åº«ç¾æœ‰ **{stats['total_chunks']}** å€‹æ–‡æª”å¡Š"
    
    if message.content:
        msg.content += f"\n\nğŸ’¬ ç¾åœ¨å›ç­”æ‚¨çš„å•é¡Œ..."
        await msg.update()
        
        # ä½¿ç”¨ä¸Šå‚³çš„æ–‡æª”å›ç­”å•é¡Œ
        response = await cl.make_async(rag_service.query_with_context)(
            message.content,
            k=4
        )
        
        msg.content += f"\n\n{response}"
    
    await msg.update()


async def _handle_text_with_rag(message: cl.Message, rag_service: RAGService):
    """ä½¿ç”¨ RAG è™•ç†ç´”æ–‡å­—è¨Šæ¯"""
    msg = cl.Message(content="")
    await msg.send()
    
    # ä½¿ç”¨ RAG æŸ¥è©¢ï¼ˆæœƒè‡ªå‹•æª¢ç´¢çŸ¥è­˜åº«ï¼‰
    response = await cl.make_async(rag_service.query_with_context)(
        message.content,
        k=4,
        use_mmr=False,
        include_sources=True
    )
    
    msg.content = response
    await msg.update()


async def _handle_image_message(
    message: cl.Message,
    image_file,
    llm_service: LLMService
):
    """è™•ç†åœ–ç‰‡è¨Šæ¯"""
    msg = cl.Message(content="ğŸ” æ­£åœ¨åˆ†æåœ–ç‰‡...")
    await msg.send()
    
    # è½‰æ›åœ–ç‰‡ç‚º data URL
    image_url = ImageService.create_image_data_url(image_file.path)
    user_text = message.content or "è«‹æè¿°é€™å¼µåœ–ç‰‡"
    
    # èª¿ç”¨ LLMï¼ˆåœ–ç‰‡ä¸ä½¿ç”¨ RAGï¼‰
    response = await cl.make_async(llm_service.send_message)(
        content=user_text,
        image_url=image_url
    )
    
    msg.content = response
    await msg.update()


@cl.action_callback("confirm")
async def on_action(action: cl.Action):
    """è™•ç†æ¸…ç©ºçŸ¥è­˜åº«ç¢ºèª"""
    rag_service = cl.user_session.get("rag_service")
    await cl.make_async(rag_service.clear_knowledge_base)()
    await cl.Message(content="âœ… çŸ¥è­˜åº«å·²æ¸…ç©º").send()


@cl.action_callback("cancel")
async def on_cancel(action: cl.Action):
    """è™•ç†å–æ¶ˆæ“ä½œ"""
    await cl.Message(content="âŒ å·²å–æ¶ˆæ“ä½œ").send()


@cl.on_settings_update
async def setup_agent(settings):
    """è™•ç†è¨­ç½®æ›´æ–°"""
    print("è¨­ç½®å·²æ›´æ–°:", settings)
