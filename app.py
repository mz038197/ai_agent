"""
Chainlit UI å±¤
è² è²¬è™•ç†ç”¨æˆ¶ç•Œé¢äº¤äº’ï¼Œå°‡æ¥­å‹™é‚è¼¯å§”æ‰˜çµ¦æœå‹™å±¤
éµå®ˆå–®ä¸€è·è²¬åŸå‰‡ (Single Responsibility Principle)
"""
import chainlit as cl
from services import (
    LLMService, 
    ImageService,
    DocumentService,
    VectorStoreService,
    RAGService,
    AgentService
)


# é…ç½®åƒæ•¸
CONFIG = {
    "MODEL": "gemma3:4b",
    "AGENT_MODEL": "qwen2.5:7b",  # Agent å°ˆç”¨æ¨¡å‹ï¼ˆéœ€æ”¯æŒå·¥å…·èª¿ç”¨ï¼‰
    "BASE_URL": "http://localhost:11434",
    "TEMPERATURE": 0.7,
    "AGENT_TEMPERATURE": 0,  # Agent å»ºè­°ç”¨è¼ƒä½æº«åº¦
    "EMBEDDING_MODEL": "nomic-embed-text",
    "CHROMA_DB_PATH": "./chroma_db",
    "ENABLE_WEB_SEARCH": True,  # æ˜¯å¦å•Ÿç”¨ç¶²è·¯æœå°‹
    "SYSTEM_PROMPT": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„ AI åŠ©æ‰‹ï¼Œå…·å‚™ä»¥ä¸‹ç‰¹é»ï¼š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
- æä¾›æº–ç¢ºã€æ¸…æ™°ã€æœ‰å¹«åŠ©çš„å›ç­”
- ç•¶è™•ç†æ–‡æª”ç›¸é—œå•é¡Œæ™‚ï¼Œåš´æ ¼åŸºæ–¼æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”
- å¦‚æœä¸ç¢ºå®šæˆ–ä¿¡æ¯ä¸è¶³ï¼Œæœƒæ˜ç¢ºèªªæ˜
- ä»¥å°ˆæ¥­ä½†è¦ªåˆ‡çš„èªæ°£èˆ‡ç”¨æˆ¶äº¤æµ"""
}


@cl.on_chat_start
async def start():
    """åˆå§‹åŒ–èŠå¤©æœƒè©±"""
    # åˆå§‹åŒ– LLM æœå‹™
    llm_service = LLMService(
        model=CONFIG["MODEL"],
        base_url=CONFIG["BASE_URL"],
        temperature=CONFIG["TEMPERATURE"],
        system_prompt=CONFIG["SYSTEM_PROMPT"]
    )
    
    # åˆå§‹åŒ– RAG ç›¸é—œæœå‹™
    doc_service = DocumentService(chunk_size=1000, chunk_overlap=200)
    vector_service = VectorStoreService(
        persist_directory=CONFIG["CHROMA_DB_PATH"],
        embedding_model=CONFIG["EMBEDDING_MODEL"],
        base_url=CONFIG["BASE_URL"]
    )
    rag_service = RAGService(
        document_service=doc_service,
        vector_store_service=vector_service,
        llm_service=llm_service,
        default_k=4
    )
    
    # åˆå§‹åŒ– Agent æœå‹™ï¼ˆæ–°å¢ï¼‰
    try:
        agent_service = AgentService(
            vector_store_service=vector_service,
            model=CONFIG["AGENT_MODEL"],
            base_url=CONFIG["BASE_URL"],
            temperature=CONFIG["AGENT_TEMPERATURE"],
            enable_web_search=CONFIG["ENABLE_WEB_SEARCH"],
            verbose=False  # åœ¨ç”Ÿç”¢ç’°å¢ƒè¨­ç‚º False
        )
        cl.user_session.set("agent_service", agent_service)
        agent_available = True
    except Exception as e:
        print(f"âš ï¸ Agent æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
        agent_available = False
    
    # å°‡æœå‹™å­˜å„²åœ¨ç”¨æˆ¶æœƒè©±ä¸­
    cl.user_session.set("llm_service", llm_service)
    cl.user_session.set("rag_service", rag_service)
    cl.user_session.set("agent_available", agent_available)
    
    # è¨­ç½®é»˜èªæ¨¡å¼ç‚º autoï¼ˆè‡ªå‹•åˆ¤æ–·ï¼‰
    cl.user_session.set("mode", "auto")
    
    # ç²å–æ¨¡å‹å’ŒçŸ¥è­˜åº«ä¿¡æ¯
    model_info = llm_service.get_model_info()
    kb_stats = rag_service.get_knowledge_base_stats()
    
    # æ­¡è¿è¨Šæ¯
    agent_status = "âœ… å·²å•Ÿç”¨" if agent_available else "âŒ æœªå•Ÿç”¨"
    agent_info = ""
    if agent_available:
        agent_service = cl.user_session.get("agent_service")
        tools = agent_service.list_tools()
        tool_names = ", ".join([t["name"] for t in tools])
        agent_info = f"\nğŸ› ï¸ **å¯ç”¨å·¥å…·:** {tool_names}\n"
    
    await cl.Message(
        content=f"ğŸ‘‹ æ­¡è¿ä½¿ç”¨ AI åŠ©æ‰‹ï¼\n\n"
                f"ğŸ“¦ **ç•¶å‰æ¨¡å‹:** {model_info['model']}\n"
                f"ğŸ“š **çŸ¥è­˜åº«:** {kb_stats['total_chunks']} å€‹æ–‡æª”å¡Š\n"
                f"ğŸ¤– **ç•¶å‰æ¨¡å¼:** è‡ªå‹•æ¨¡å¼ (auto)\n"
                f"ğŸ¤– **Agent æ¨¡å¼:** {agent_status}{agent_info}\n"
                f"ğŸ’¬ **æ‚¨å¯ä»¥ï¼š**\n"
                f"- ğŸ’­ è¼¸å…¥æ–‡å­—é€²è¡Œå°è©±\n"
                f"- ğŸ“„ ä¸Šå‚³æ–‡ä»¶ï¼ˆPDF/TXT/Markdownï¼‰å»ºç«‹çŸ¥è­˜åº«\n"
                f"- ğŸ–¼ï¸ ä¸Šå‚³åœ–ç‰‡é€²è¡Œè¦–è¦ºåˆ†æ\n\n"
                f"âš™ï¸ **æ¨¡å¼åˆ‡æ›ï¼š**\n"
                f"- `/auto` - è‡ªå‹•åˆ¤æ–·æ˜¯å¦ä½¿ç”¨çŸ¥è­˜åº«ï¼ˆé è¨­ï¼‰\n"
                f"- `/chat` - ç´”èŠå¤©æ¨¡å¼ï¼ˆä¸ä½¿ç”¨çŸ¥è­˜åº«ï¼‰\n"
                f"- `/rag` - çŸ¥è­˜åº«æ¨¡å¼ï¼ˆå¼·åˆ¶æª¢ç´¢æ–‡æª”ï¼‰\n"
                + (f"- `/agent` - Agent æ¨¡å¼ï¼ˆLLM è‡ªä¸»èª¿ç”¨å·¥å…·ï¼‰\n" if agent_available else "")
                + f"\nğŸ“‹ **å…¶ä»–å‘½ä»¤ï¼š**\n"
                f"- `/stats` - æŸ¥çœ‹çŸ¥è­˜åº«çµ±è¨ˆ\n"
                f"- `/clear` - æ¸…ç©ºçŸ¥è­˜åº«",
    ).send()


@cl.on_message
async def handle_message(message: cl.Message):
    """
    è™•ç†ç”¨æˆ¶è¨Šæ¯ï¼ˆçµ±ä¸€è™•ç†æ–‡å­—ã€åœ–ç‰‡å’Œæ–‡æª”ï¼‰
    UIå±¤åªè² è²¬æ¥æ”¶è¼¸å…¥ã€é¡¯ç¤ºè¼¸å‡ºï¼Œæ¥­å‹™é‚è¼¯å§”æ‰˜çµ¦æœå‹™å±¤
    """
    # ç²å–æœå‹™å±¤å¯¦ä¾‹
    llm_service = cl.user_session.get("llm_service")
    rag_service = cl.user_session.get("rag_service")
    
    # æª¢æŸ¥å‘½ä»¤
    if message.content:
        content_lower = message.content.lower().strip()
        
        # æ¨¡å¼åˆ‡æ›å‘½ä»¤
        if content_lower == "/chat":
            cl.user_session.set("mode", "chat")
            await cl.Message(
                content="ğŸ’¬ **å·²åˆ‡æ›åˆ°èŠå¤©æ¨¡å¼**\n\n"
                        "- ä¸æœƒæª¢ç´¢çŸ¥è­˜åº«\n"
                        "- ç´”ç²¹èˆ‡ AI å°è©±\n"
                        "- é©åˆé–’èŠã€å¸¸è­˜å•é¡Œ"
            ).send()
            return
        
        if content_lower == "/rag":
            cl.user_session.set("mode", "rag")
            await cl.Message(
                content="ğŸ“š **å·²åˆ‡æ›åˆ°çŸ¥è­˜åº«æ¨¡å¼**\n\n"
                        "- å¼·åˆ¶æª¢ç´¢çŸ¥è­˜åº«\n"
                        "- åŸºæ–¼æ–‡æª”å…§å®¹å›ç­”\n"
                        "- é©åˆæŸ¥è©¢å·²ä¸Šå‚³çš„æ–‡æª”"
            ).send()
            return
        
        if content_lower == "/auto":
            cl.user_session.set("mode", "auto")
            await cl.Message(
                content="ğŸ¤– **å·²åˆ‡æ›åˆ°è‡ªå‹•æ¨¡å¼**\n\n"
                        "- æ™ºèƒ½åˆ¤æ–·æ˜¯å¦éœ€è¦çŸ¥è­˜åº«\n"
                        "- æ ¹æ“šå•é¡Œç›¸é—œæ€§è‡ªå‹•é¸æ“‡\n"
                        "- é©åˆæ··åˆä½¿ç”¨å ´æ™¯ï¼ˆé è¨­ï¼‰"
            ).send()
            return
        
        if content_lower == "/agent":
            agent_available = cl.user_session.get("agent_available", False)
            if not agent_available:
                await cl.Message(
                    content="âŒ **Agent æ¨¡å¼ä¸å¯ç”¨**\n\n"
                            "å¯èƒ½åŸå› ï¼š\n"
                            "- Agent æ¨¡å‹æœªä¸‹è¼‰ï¼ˆéœ€è¦ qwen2.5:7b æˆ–å…¶ä»–æ”¯æŒå·¥å…·èª¿ç”¨çš„æ¨¡å‹ï¼‰\n"
                            "- ç¶²è·¯æœå°‹å·¥å…·åˆå§‹åŒ–å¤±æ•—\n\n"
                            "è«‹ç¢ºä¿å·²ä¸‹è¼‰æ”¯æŒå·¥å…·èª¿ç”¨çš„æ¨¡å‹ï¼š\n"
                            "`ollama pull qwen2.5:7b`"
                ).send()
                return
            
            cl.user_session.set("mode", "agent")
            agent_service = cl.user_session.get("agent_service")
            tools = agent_service.list_tools()
            tools_info = "\n".join([f"  â€¢ **{t['name']}**: {t['description']}" for t in tools])
            
            await cl.Message(
                content=f"ğŸ¤– **å·²åˆ‡æ›åˆ° Agent æ¨¡å¼**\n\n"
                        f"- LLM è‡ªä¸»æ±ºå®šä½•æ™‚ä½¿ç”¨å·¥å…·\n"
                        f"- æ”¯æ´çŸ¥è­˜åº«æª¢ç´¢ + ç¶²è·¯æœå°‹\n"
                        f"- é©åˆè¤‡é›œæŸ¥è©¢å’Œå¤šæ­¥æ¨ç†\n\n"
                        f"**å¯ç”¨å·¥å…·ï¼š**\n{tools_info}"
            ).send()
            return
        
        # çµ±è¨ˆå‘½ä»¤
        if content_lower == "/stats":
            current_mode = cl.user_session.get("mode", "auto")
            mode_emoji = {"chat": "ğŸ’¬", "rag": "ğŸ“š", "auto": "ğŸ¤–", "agent": "ğŸ¤–"}
            mode_name = {"chat": "èŠå¤©æ¨¡å¼", "rag": "çŸ¥è­˜åº«æ¨¡å¼", "auto": "è‡ªå‹•æ¨¡å¼", "agent": "Agent æ¨¡å¼"}
            
            stats = rag_service.get_knowledge_base_stats()
            
            stats_content = (
                f"ğŸ“Š **ç³»çµ±ç‹€æ…‹**\n\n"
                f"ğŸ¤– **ç•¶å‰æ¨¡å¼:** {mode_emoji.get(current_mode, 'ğŸ¤–')} {mode_name.get(current_mode, 'è‡ªå‹•æ¨¡å¼')}\n\n"
                f"ğŸ“š **çŸ¥è­˜åº«çµ±è¨ˆï¼š**\n"
                f"- æ–‡æª”å¡Šç¸½æ•¸ï¼š{stats['total_chunks']}\n"
                f"- é›†åˆåç¨±ï¼š{stats['collection_name']}\n"
                f"- åµŒå…¥æ¨¡å‹ï¼š{stats['embedding_model']}\n"
                f"- æ”¯æ´æ ¼å¼ï¼š{', '.join(stats['supported_formats'])}"
            )
            
            # å¦‚æœæ˜¯ Agent æ¨¡å¼ï¼Œé¡¯ç¤º Agent ä¿¡æ¯
            if current_mode == "agent":
                agent_service = cl.user_session.get("agent_service")
                if agent_service:
                    agent_info = agent_service.get_agent_info()
                    stats_content += (
                        f"\n\nğŸ¤– **Agent é…ç½®ï¼š**\n"
                        f"- æ¨¡å‹ï¼š{agent_info['model']}\n"
                        f"- æº«åº¦ï¼š{agent_info['temperature']}\n"
                        f"- å·¥å…·ï¼š{', '.join(agent_info['tools'])}"
                    )
            
            await cl.Message(content=stats_content).send()
            return
        
        # æ¸…ç©ºå‘½ä»¤
        if content_lower == "/clear":
            await cl.AskActionMessage(
                content="ç¢ºå®šè¦æ¸…ç©ºæ•´å€‹çŸ¥è­˜åº«å—ï¼Ÿæ­¤æ“ä½œç„¡æ³•æ’¤éŠ·ã€‚",
                actions=[
                    cl.Action(name="confirm", payload={"action": "confirm"}, label="âœ… ç¢ºå®šæ¸…ç©º"),
                    cl.Action(name="cancel", payload={"action": "cancel"}, label="âŒ å–æ¶ˆ"),
                ],
            ).send()
            return
    
    # åˆ†é¡é™„ä»¶
    images = [file for file in message.elements if "image" in file.mime]
    documents = [file for file in message.elements 
                 if file.mime in ["application/pdf", "text/plain", "text/markdown"]
                 or file.name.endswith(('.pdf', '.txt', '.md', '.markdown'))]
    
    try:
        # è™•ç†æ–‡æª”ä¸Šå‚³
        if documents:
            await _handle_document_upload(message, documents, rag_service)
        
        # è™•ç†åœ–ç‰‡
        elif images:
            await _handle_image_message(message, images[0], llm_service)
        
        # è™•ç†ç´”æ–‡å­—ï¼ˆæ ¹æ“šæ¨¡å¼é¸æ“‡è™•ç†æ–¹å¼ï¼‰
        else:
            await _handle_text_with_rag(message, rag_service, llm_service)
            
    except Exception as e:
        await cl.Message(
            content=f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}\n\nè«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ¨¡å‹å·²ä¸‹è¼‰ã€‚"
        ).send()


async def _handle_document_upload(
    message: cl.Message,
    documents: list,
    rag_service: RAGService
):
    """è™•ç†æ–‡æª”ä¸Šå‚³"""
    msg = cl.Message(content="ğŸ“„ æ­£åœ¨è™•ç†æ–‡ä»¶...")
    await msg.send()
    
    results = []
    for doc_file in documents:
        try:
            result = await cl.make_async(rag_service.ingest_file)(doc_file.path)
            results.append(f"âœ… **{doc_file.name}**\n   - å·²æ·»åŠ  {result['chunks_count']} å€‹æ–‡æª”å¡Š")
        except Exception as e:
            results.append(f"âŒ **{doc_file.name}**\n   - éŒ¯èª¤ï¼š{str(e)}")
    
    # ç²å–æ›´æ–°å¾Œçš„çµ±è¨ˆ
    stats = rag_service.get_knowledge_base_stats()
    
    msg.content = "ğŸ“š **æ–‡æª”è™•ç†å®Œæˆ**\n\n" + "\n\n".join(results)
    msg.content += f"\n\nğŸ“Š çŸ¥è­˜åº«ç¾æœ‰ **{stats['total_chunks']}** å€‹æ–‡æª”å¡Š"
    
    if message.content:
        msg.content += f"\n\nğŸ’¬ ç¾åœ¨å›ç­”æ‚¨çš„å•é¡Œ..."
        await msg.update()
        
        # ä½¿ç”¨ä¸Šå‚³çš„æ–‡æª”å›ç­”å•é¡Œ
        response = await cl.make_async(rag_service.query_with_context)(
            message.content,
            k=4
        )
        
        msg.content += f"\n\n{response}"
    
    await msg.update()


async def _handle_text_with_rag(
    message: cl.Message, 
    rag_service: RAGService,
    llm_service: LLMService
):
    """æ ¹æ“šç•¶å‰æ¨¡å¼è™•ç†ç´”æ–‡å­—è¨Šæ¯"""
    msg = cl.Message(content="")
    await msg.send()
    
    # ç²å–ç•¶å‰æ¨¡å¼
    mode = cl.user_session.get("mode", "auto")
    
    # æ ¹æ“šæ¨¡å¼é¸æ“‡è™•ç†æ–¹å¼
    if mode == "chat":
        # ç´”èŠå¤©æ¨¡å¼ - ä¸æª¢ç´¢çŸ¥è­˜åº«
        response = await cl.make_async(llm_service.send_message)(message.content)
    
    elif mode == "rag":
        # çŸ¥è­˜åº«æ¨¡å¼ - å¼·åˆ¶ä½¿ç”¨ RAG
        response = await cl.make_async(rag_service.query_with_context)(
            message.content,
            k=4,
            use_mmr=False,
            include_sources=True
        )
    
    elif mode == "agent":
        # Agent æ¨¡å¼ - LLM è‡ªä¸»èª¿ç”¨å·¥å…·
        agent_service = cl.user_session.get("agent_service")
        if agent_service:
            msg.content = "ğŸ¤– Agent æ­£åœ¨æ€è€ƒä¸¦æ±ºå®šä½¿ç”¨å“ªäº›å·¥å…·..."
            await msg.update()
            
            result = await cl.make_async(agent_service.query)(message.content)
            
            if result["success"]:
                response = result["answer"]
                
                # é¡¯ç¤ºä½¿ç”¨çš„å·¥å…·ï¼ˆå¯é¸ï¼‰
                if result.get("intermediate_steps"):
                    steps_info = "\n\n---\n*ä½¿ç”¨çš„å·¥å…·: "
                    tools_used = set()
                    for step in result["intermediate_steps"]:
                        if hasattr(step[0], 'tool'):
                            tools_used.add(step[0].tool)
                    if tools_used:
                        response += steps_info + ", ".join(tools_used) + "*"
            else:
                response = result["answer"]
        else:
            response = "âŒ Agent æœå‹™ä¸å¯ç”¨"
    
    else:  # auto æ¨¡å¼
        # è‡ªå‹•åˆ¤æ–·æ¨¡å¼ - ä½¿ç”¨æ™ºèƒ½æŸ¥è©¢
        response = await cl.make_async(rag_service.query_with_auto_mode)(
            message.content,
            k=4,
            include_sources=True
        )
    
    msg.content = response
    await msg.update()


async def _handle_image_message(
    message: cl.Message,
    image_file,
    llm_service: LLMService
):
    """è™•ç†åœ–ç‰‡è¨Šæ¯"""
    msg = cl.Message(content="ğŸ” æ­£åœ¨åˆ†æåœ–ç‰‡...")
    await msg.send()
    
    # è½‰æ›åœ–ç‰‡ç‚º data URL
    image_url = ImageService.create_image_data_url(image_file.path)
    user_text = message.content or "è«‹æè¿°é€™å¼µåœ–ç‰‡"
    
    # èª¿ç”¨ LLMï¼ˆåœ–ç‰‡ä¸ä½¿ç”¨ RAGï¼‰
    response = await cl.make_async(llm_service.send_message)(
        content=user_text,
        image_url=image_url
    )
    
    msg.content = response
    await msg.update()


@cl.action_callback("confirm")
async def on_action(action: cl.Action):
    """è™•ç†æ¸…ç©ºçŸ¥è­˜åº«ç¢ºèª"""
    rag_service = cl.user_session.get("rag_service")
    await cl.make_async(rag_service.clear_knowledge_base)()
    await cl.Message(content="âœ… çŸ¥è­˜åº«å·²æ¸…ç©º").send()


@cl.action_callback("cancel")
async def on_cancel(action: cl.Action):
    """è™•ç†å–æ¶ˆæ“ä½œ"""
    await cl.Message(content="âŒ å·²å–æ¶ˆæ“ä½œ").send()


@cl.on_settings_update
async def setup_agent(settings):
    """è™•ç†è¨­ç½®æ›´æ–°"""
    print("è¨­ç½®å·²æ›´æ–°:", settings)
